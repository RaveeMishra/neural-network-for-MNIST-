{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2f793f4",
        "outputId": "e0d3b6c1-29af-4852-c74d-77b8b74a407c"
      },
      "source": [
        "import numpy as np\n",
        "# import gzip # No longer needed for uncompressed .ubyte files\n",
        "\n",
        "def read_mnist_images(file_path):\n",
        "    # Use standard open() for uncompressed .ubyte files\n",
        "    with open(file_path, 'rb') as f:\n",
        "        magic_number = int.from_bytes(f.read(4), 'big')\n",
        "        num_images = int.from_bytes(f.read(4), 'big')\n",
        "        num_rows = int.from_bytes(f.read(4), 'big')\n",
        "        num_cols = int.from_bytes(f.read(4), 'big')\n",
        "\n",
        "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "        images = images.reshape(num_images, num_rows, num_cols)\n",
        "    return images\n",
        "\n",
        "def read_mnist_labels(file_path):\n",
        "    # Use standard open() for uncompressed .ubyte files\n",
        "    with open(file_path, 'rb') as f:\n",
        "        magic_number = int.from_bytes(f.read(4), 'big')\n",
        "        num_labels = int.from_bytes(f.read(4), 'big')\n",
        "\n",
        "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "    return labels\n",
        "\n",
        "# Read training images and labels\n",
        "train_images = read_mnist_images('train-images.idx3-ubyte')\n",
        "train_labels = read_mnist_labels('train-labels.idx1-ubyte')\n",
        "\n",
        "print(f\"Shape of training images: {train_images.shape}\")\n",
        "print(f\"Data type of training images: {train_images.dtype}\")\n",
        "print(f\"Shape of training labels: {train_labels.shape}\")\n",
        "print(f\"Data type of training labels: {train_labels.dtype}\")\n",
        "\n",
        "# Read test images and labels\n",
        "test_images = read_mnist_images('t10k-images.idx3-ubyte')\n",
        "test_labels = read_mnist_labels('t10k-labels.idx1-ubyte')\n",
        "\n",
        "print(f\"Shape of test images: {test_images.shape}\")\n",
        "print(f\"Data type of test images: {test_images.dtype}\")\n",
        "print(f\"Shape of test labels: {test_labels.shape}\")\n",
        "print(f\"Data type of test labels: {test_labels.dtype}\")\n",
        "\n",
        "\n",
        "print(train_images[0].shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training images: (60000, 28, 28)\n",
            "Data type of training images: uint8\n",
            "Shape of training labels: (60000,)\n",
            "Data type of training labels: uint8\n",
            "Shape of test images: (10000, 28, 28)\n",
            "Data type of test images: uint8\n",
            "Shape of test labels: (10000,)\n",
            "Data type of test labels: uint8\n",
            "(28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "dv7Gt1ba8u-Q"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural_network_for_MNIST(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Neural_network_for_MNIST, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Convolution layer C1: 1 input image channel, 6 output channels,\n",
        "        # 5x5 square convolution, it uses RELU activation function, and\n",
        "        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch\n",
        "        c1 = F.relu(self.conv1(input))\n",
        "        # Subsampling layer S2: 2x2 grid, purely functional,\n",
        "        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor\n",
        "        s2 = F.max_pool2d(c1, (2, 2))\n",
        "        # Convolution layer C3: 6 input channels, 16 output channels,\n",
        "        # 5x5 square convolution, it uses RELU activation function, and\n",
        "        # outputs a (N, 16, 10, 10) Tensor\n",
        "        c3 = F.relu(self.conv2(s2))\n",
        "        # Subsampling layer S4: 2x2 grid, purely functional,\n",
        "        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor\n",
        "        s4 = F.max_pool2d(c3, 2)\n",
        "        # Flatten operation: purely functional, outputs a (N, 400) Tensor\n",
        "        s4 = torch.flatten(s4, 1)\n",
        "        # Fully connected layer F5: (N, 400) Tensor input,\n",
        "        # and outputs a (N, 120) Tensor, it uses RELU activation function\n",
        "        f5 = F.relu(self.fc1(s4))\n",
        "        # Fully connected layer F6: (N, 120) Tensor input,\n",
        "        # and outputs a (N, 84) Tensor, it uses RELU activation function\n",
        "        f6 = F.relu(self.fc2(f5))\n",
        "        # Fully connected layer OUTPUT: (N, 84) Tensor input, and\n",
        "        # outputs a (N, 10) Tensor\n",
        "        output = self.fc3(f6)\n",
        "        return output\n",
        "\n",
        "\n",
        "net = Neural_network_for_MNIST()\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdtK_NmV-GqD",
        "outputId": "9ce12b18-d99c-40ce-85e0-abbd40db8526"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural_network_for_MNIST(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pR61BcQ-Jxz",
        "outputId": "21a7361c-24cf-4ef7-8fd9-232683ad3598"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "torch.Size([6, 1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)\n",
        "\n",
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30PVdHMh_Edx",
        "outputId": "de25128f-d780-478c-fef3-317e299d51b8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0450,  0.0435,  0.0522,  0.0424, -0.0679,  0.0031,  0.0261, -0.0732,\n",
            "         -0.0785, -0.0592]], grad_fn=<AddmmBackward0>)\n",
            "tensor(0.4460, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))\n"
      ],
      "metadata": {
        "id": "ww53dPdaDoJy"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR7JGKkdD-yY",
        "outputId": "725aff23-498d-452c-ffff-c9582d22a162"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.7622, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWkyUYBrEC85",
        "outputId": "15f8cbbc-d65a-45ca-8a6a-96e6e0aae250"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.bias.grad before backward\n",
            "None\n",
            "conv1.bias.grad after backward\n",
            "tensor([ 0.0174, -0.0264,  0.0110, -0.0085, -0.0126,  0.0059])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "    f.data.sub_(f.grad.data * learning_rate)"
      ],
      "metadata": {
        "id": "KyYeAqQJEUMN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ],
      "metadata": {
        "id": "gv3rALXuEW-u"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rOfnQ2S1Ef-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WXjWsi0JGwYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ojG8-7tHGwWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xIREltDtGwVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1cB1TeYsGwTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a-MVkNTmGwR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4l8BFcoZGwQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For cuda (GPU) device\n"
      ],
      "metadata": {
        "id": "qerea3CPGyy5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42b37c57",
        "outputId": "6ff9a71d-38b7-4e70-fd8e-fdfba8e99666"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 1. Convert NumPy arrays to PyTorch tensors\n",
        "train_images_tensor = torch.from_numpy(train_images.copy()).float()\n",
        "train_labels_tensor = torch.from_numpy(train_labels.copy()).long()\n",
        "test_images_tensor = torch.from_numpy(test_images.copy()).float()\n",
        "test_labels_tensor = torch.from_numpy(test_labels.copy()).long()\n",
        "\n",
        "# 2. Normalize the image tensors (pixel values between 0 and 1)\n",
        "train_images_tensor /= 255.0\n",
        "test_images_tensor /= 255.0\n",
        "\n",
        "# 3. Reshape the normalized image tensors to add a channel dimension (N, C, H, W)\n",
        "train_images_tensor = train_images_tensor.unsqueeze(1) # Adds a channel dimension at index 1\n",
        "test_images_tensor = test_images_tensor.unsqueeze(1)\n",
        "\n",
        "# 4. Create TensorDataset objects\n",
        "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
        "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
        "\n",
        "# 5. Create DataLoader objects\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Shape of training images tensor: {train_images_tensor.shape}\")\n",
        "print(f\"Shape of training labels tensor: {train_labels_tensor.shape}\")\n",
        "print(f\"Shape of test images tensor: {test_images_tensor.shape}\")\n",
        "print(f\"Shape of test labels tensor: {test_labels_tensor.shape}\")\n",
        "print(f\"Number of batches in training DataLoader: {len(train_loader)}\")\n",
        "print(f\"Number of batches in test DataLoader: {len(test_loader)}\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training images tensor: torch.Size([60000, 1, 28, 28])\n",
            "Shape of training labels tensor: torch.Size([60000])\n",
            "Shape of test images tensor: torch.Size([10000, 1, 28, 28])\n",
            "Shape of test labels tensor: torch.Size([10000])\n",
            "Number of batches in training DataLoader: 938\n",
            "Number of batches in test DataLoader: 157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82fe4ae9",
        "outputId": "398f4f2a-3a8a-4985-9a69-26773b4e971a"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() # Set the model to training mode\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # Use F.nll_loss as specified, which expects log-probabilities for output\n",
        "        # Our model output is linear, so we need to apply log_softmax to it first.\n",
        "        loss = F.nll_loss(F.log_softmax(output, dim=1), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0: # Print training loss every 100 batches\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "print(\"Train function defined.\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d61c7848",
        "outputId": "6e1d5d69-1aa0-4b60-ab85-245c99eb3edc"
      },
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            # Sum up batch loss\n",
        "            test_loss += F.nll_loss(F.log_softmax(output, dim=1), target, reduction='sum').item()\n",
        "            # Get the index of the max log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "print(\"Test function defined.\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01ee9f32",
        "outputId": "8f79b12e-9eea-4364-ced2-c1d59379fa0f"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# 1. Determine the device to use for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 2. Instantiate the Neural_network_for_MNIST model and move it to the device\n",
        "model = Neural_network_for_MNIST().to(device)\n",
        "print(\"Model instantiated and moved to device.\")\n",
        "\n",
        "# 3. Define the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "print(\"Optimizer defined.\")\n",
        "\n",
        "# 4. Set the number of training epochs\n",
        "epochs = 10\n",
        "print(f\"Number of epochs set to: {epochs}\")\n",
        "\n",
        "# 5. Implement the training and evaluation loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "print(\"Training and evaluation complete.\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Model instantiated and moved to device.\n",
            "Optimizer defined.\n",
            "Number of epochs set to: 10\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299490\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.303595\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.286077\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.294536\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.277275\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.273900\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.252067\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.211722\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.111421\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.523725\n",
            "\n",
            "Test set: Average loss: 1.3625, Accuracy: 6416/10000 (64%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.424294\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.770885\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.434408\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.452551\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.671873\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.435312\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.327943\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.252289\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.183142\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.344667\n",
            "\n",
            "Test set: Average loss: 0.2812, Accuracy: 9183/10000 (92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.375160\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.344871\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.271527\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.375956\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.204503\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.317779\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.215702\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.245516\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.071465\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.167608\n",
            "\n",
            "Test set: Average loss: 0.4911, Accuracy: 8478/10000 (85%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.494435\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.144317\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.069546\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.155591\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.139741\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.154344\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.136863\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.174544\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.154526\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.043499\n",
            "\n",
            "Test set: Average loss: 0.1310, Accuracy: 9573/10000 (96%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.067324\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.064905\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.107848\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.252403\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.199308\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.140173\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.091189\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.072484\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.114312\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.067070\n",
            "\n",
            "Test set: Average loss: 0.0989, Accuracy: 9703/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.066646\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.048786\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.154089\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.040856\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.192276\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.053688\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.054144\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.026730\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.042234\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.073543\n",
            "\n",
            "Test set: Average loss: 0.0914, Accuracy: 9715/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.059952\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.056813\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.031713\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.022783\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.179167\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.278554\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.033714\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.126774\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.033516\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.082319\n",
            "\n",
            "Test set: Average loss: 0.0777, Accuracy: 9755/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.066643\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.048145\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.080003\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.075492\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.083739\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.025051\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.067617\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.148860\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.068221\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.129757\n",
            "\n",
            "Test set: Average loss: 0.0739, Accuracy: 9765/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.139695\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.055694\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.073523\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.043906\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.040774\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.076970\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.035788\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.091700\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.044655\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.034500\n",
            "\n",
            "Test set: Average loss: 0.0648, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.066333\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.168040\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.066363\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.014759\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.129689\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.062884\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.168115\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.033236\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.083146\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.024978\n",
            "\n",
            "Test set: Average loss: 0.0617, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Training and evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcf355b7"
      },
      "source": [
        "## Final Accuracy Summary\n",
        "\n",
        "After 10 epochs of training, the model achieved a final accuracy of **98%** on the test set, with an average loss of 0.0617. This indicates that the neural network is performing well on the MNIST digit classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56da9d6f"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the training and evaluation results, including the final accuracy achieved on the MNIST dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b68ad7c0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The final accuracy achieved on the MNIST dataset after training for 10 epochs is 98%, with an average test loss of 0.0617.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Data Preparation:** MNIST images were successfully converted to PyTorch tensors, normalized to the 0-1 range, and reshaped to include a channel dimension (e.g., `torch.Size([60000, 1, 28, 28])` for training images). DataLoaders were then created with a batch size of 64, resulting in 938 training batches and 157 test batches.\n",
        "*   **Model Architecture Correction:** An initial `RuntimeError` during training indicated a shape mismatch in the `Neural_network_for_MNIST` model. The first fully connected layer (`self.fc1`) was incorrectly configured to expect 400 input features, while the preceding convolutional and pooling layers produced 256 features. This was corrected by changing the input feature size for `self.fc1` from `16 * 5 * 5` to `16 * 4 * 4`.\n",
        "*   **Training and Evaluation Success:** After correcting the model architecture, the network was successfully trained for 10 epochs.\n",
        "*   **Final Performance:** The trained neural network achieved a final accuracy of 98% on the test set, with an average loss of 0.0617.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The model demonstrates strong performance on the MNIST dataset, achieving a high accuracy of 98% with a relatively low loss, indicating it has effectively learned to classify handwritten digits.\n",
        "*   Future steps could involve exploring hyperparameter tuning (e.g., learning rate, optimizer variants), data augmentation, or more complex network architectures to potentially achieve even higher accuracy.\n"
      ]
    }
  ]
}